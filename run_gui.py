"""
Juniper Syslog Filter - GUIç‰ˆï¼ˆStreamlitï¼‰

å®Ÿè¡Œæ–¹æ³•:
    streamlit run run_gui.py
"""

import streamlit as st
from pathlib import Path
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from modules.extract import extract_zip
from modules.filter_keyword import filter_keyword
from modules.cleanup_temp import cleanup_processed_files
from modules.merge_files import merge_csv_files
from modules.reduce_columns import reduce_columns
from modules.extract_routing import extract_routing
from modules.split_ip import split_ip
from modules.classify_ip import classify_ip
from modules.extract_protocol import extract_protocol
from modules.extract_severity_level import extract_severity_level
from modules.extract_severity import extract_severity
from modules.filter_critical_and_merge import filter_and_merge_critical
from modules.export_excel import export_to_excel
from modules.cleanup_all import cleanup_all_directories


def main():
    """
    Streamlit GUI ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    st.set_page_config(
        page_title="Juniper Syslog Filter", page_icon="ğŸ”¥", layout="wide"
    )

    st.title("ğŸ”¥ Juniper Syslog Filter")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
        keyword = st.text_input(
            "ãƒ•ã‚£ãƒ«ã‚¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            value="RT_IDP_ATTACK",
            help="ãƒ­ã‚°ã‹ã‚‰æŠ½å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŒ‡å®š",
        )

        # Severityãƒ•ã‚£ãƒ«ã‚¿
        severity_filter = st.selectbox(
            "Severityãƒ•ã‚£ãƒ«ã‚¿",
            options=["CRITICAL", "WARNING", "INFO"],
            index=0,
            help="æŠ½å‡ºã™ã‚‹Severityãƒ¬ãƒ™ãƒ«ã‚’é¸æŠ",
        )

        st.markdown("---")

        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        run_button = st.button("ğŸš€ å®Ÿè¡Œ", type="primary", use_container_width=True)

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("ğŸ“Š å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
        status_placeholder = st.empty()

    with col2:
        st.subheader("ğŸ“ˆ çµ±è¨ˆæƒ…å ±")
        stats_placeholder = st.empty()

    # å®Ÿè¡Œå‡¦ç†
    if run_button:
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹è¨­å®š
            source_dir = project_root / "source_logs"
            temp_dir = project_root / "temp_extracted"
            filtered_dir = project_root / "filtered_logs"
            merged_dir = project_root / "merged_logs"
            reduced_dir = project_root / "reduced_logs"
            routed_dir = project_root / "routed_logs"
            splitted_dir = project_root / "splitted_logs"
            classified_dir = project_root / "classified_logs"
            protocol_dir = project_root / "protocol_extracted"
            severity_dir = project_root / "severity_level_extracted"
            severity_extracted_dir = project_root / "severity_extracted"
            critical_dir = project_root / "critical_only"
            final_output_dir = project_root / "final_output"

            with st.spinner("å‡¦ç†ä¸­..."):
                # çµ±è¨ˆæƒ…å ±
                stats = {"å‡¦ç†æ¸ˆã¿ZIP": 0, "ãƒ•ã‚£ãƒ«ã‚¿è¡Œæ•°": 0, "æœ€çµ‚å‡ºåŠ›è¡Œæ•°": 0}

                # Phase 1: ãƒ«ãƒ¼ãƒ—å‡¦ç†
                status_placeholder.info("ğŸ”„ Phase 1: ZIPå±•é–‹ + ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")

                processed_count = 0
                total_filtered = 0

                while True:
                    zip_files = sorted(source_dir.glob("*.zip"))
                    if not zip_files:
                        break

                    current_zip = zip_files[0]
                    extracted_csvs = extract_zip(current_zip, temp_dir)
                    filtered_count = filter_keyword(
                        extracted_csvs, filtered_dir, keyword=keyword
                    )
                    cleanup_processed_files(current_zip, extracted_csvs, verbose=False)

                    processed_count += 1
                    total_filtered += filtered_count

                    # çµ±è¨ˆæ›´æ–°
                    stats["å‡¦ç†æ¸ˆã¿ZIP"] = processed_count
                    stats["ãƒ•ã‚£ãƒ«ã‚¿è¡Œæ•°"] = total_filtered
                    stats_placeholder.json(stats)

                # Phase 2: ãƒãƒ¼ã‚¸
                status_placeholder.info("ğŸ”„ Phase 2: ãƒãƒ¼ã‚¸å‡¦ç†")
                filtered_files = sorted(filtered_dir.glob("*.csv"))
                if filtered_files:
                    merged_files = merge_csv_files(
                        filtered_files, merged_dir, max_rows=800000, verbose=False
                    )

                # Phase 3-11: å¤‰æ›å‡¦ç†ï¼ˆç°¡ç•¥è¡¨ç¤ºï¼‰
                status_placeholder.info("ğŸ”„ Phase 3-11: ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†")

                # åˆ—å‰Šé™¤
                merged_files = sorted(merged_dir.glob("*.csv"))
                if merged_files:
                    reduced_files = reduce_columns(
                        merged_files,
                        reduced_dir,
                        keep_columns=[0, 1, 2, 6],
                        verbose=False,
                    )

                # routingæŠ½å‡º
                reduced_files = sorted(reduced_dir.glob("*.csv"))
                if reduced_files:
                    routed_files = extract_routing(
                        reduced_files, routed_dir, verbose=False
                    )

                # IPåˆ†å‰²
                routed_files = sorted(routed_dir.glob("*.csv"))
                if routed_files:
                    splitted_files = split_ip(routed_files, splitted_dir, verbose=False)

                # IPåˆ†é¡
                splitted_files = sorted(splitted_dir.glob("*.csv"))
                if splitted_files:
                    classified_files = classify_ip(
                        splitted_files, classified_dir, verbose=False
                    )

                # protocolæŠ½å‡º
                classified_files = sorted(classified_dir.glob("*.csv"))
                if classified_files:
                    protocol_files = extract_protocol(
                        classified_files, protocol_dir, verbose=False
                    )

                # SeverityLevelæŠ½å‡º
                protocol_files = sorted(protocol_dir.glob("*.csv"))
                if protocol_files:
                    severity_level_files = extract_severity_level(
                        protocol_files, severity_dir, verbose=False
                    )

                # SeverityæŠ½å‡º
                severity_level_files = sorted(severity_dir.glob("*.csv"))
                if severity_level_files:
                    severity_extracted_files = extract_severity(
                        severity_level_files, severity_extracted_dir, verbose=False
                    )

                # Phase 10: CRITICALæŠ½å‡º + ãƒãƒ¼ã‚¸
                status_placeholder.info(f"ğŸ”„ Phase 10: {severity_filter}æŠ½å‡º + ãƒãƒ¼ã‚¸")
                severity_extracted_files = sorted(severity_extracted_dir.glob("*.csv"))
                if severity_extracted_files:
                    critical_output = critical_dir / "critical_merged.csv"
                    result = filter_and_merge_critical(
                        severity_extracted_files,
                        critical_output,
                        severity_filter=severity_filter,
                        verbose=False,
                    )

                    if result:
                        # æœ€çµ‚è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        import pandas as pd

                        df = pd.read_csv(result)
                        stats["æœ€çµ‚å‡ºåŠ›è¡Œæ•°"] = len(df)
                        stats_placeholder.json(stats)

                # Phase 11: Excelå‡ºåŠ›
                status_placeholder.info("ğŸ”„ Phase 11: Excelå‡ºåŠ›")
                critical_file = critical_dir / "critical_merged.csv"
                if critical_file.exists():
                    excel_output = export_to_excel(
                        critical_file, final_output_dir, verbose=False
                    )

                # Phase 12: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                status_placeholder.info("ğŸ”„ Phase 12: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
                cleanup_all_directories(project_root, verbose=False)

                # å®Œäº†
                status_placeholder.success(f"âœ… å‡¦ç†å®Œäº†ï¼å‡ºåŠ›: {excel_output.name}")
                st.balloons()

        except Exception as e:
            status_placeholder.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.exception(e)

    else:
        # åˆæœŸçŠ¶æ…‹
        status_placeholder.info("â¸ï¸ è¨­å®šã‚’ç¢ºèªã—ã¦ã€Œå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")

        # èª¬æ˜
        st.markdown("### ğŸ“ ä½¿ã„æ–¹")
        st.markdown(
            """
        1. **source_logs/** ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®
        2. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã‚’ç¢ºèª
        3. ã€Œå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        4. å‡¦ç†å®Œäº†å¾Œã€**final_output/** ã«Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡ºåŠ›ã•ã‚Œã¾ã™
        """
        )

        st.markdown("### ğŸ“‹ ç¾åœ¨ã®è¨­å®š")
        st.code(
            f"""
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}
Severityãƒ•ã‚£ãƒ«ã‚¿: {severity_filter}
        """
        )


if __name__ == "__main__":
    main()
